{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "Requirement already satisfied: opencv-python in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n",
      "Requirement already satisfied: gym-super-mario-bros in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from gym-super-mario-bros) (8.2.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.23.5)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.5.21)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/bozhiepokorny/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "#only run once\n",
    "#!pip install nes-py==0.2.6\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "!apt install -y libgl1-mesa-glx\n",
    "!pip install opencv-python\n",
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from toolkit.gym_env import *\n",
    "from toolkit.baseline_model import *\n",
    "from toolkit.gym_env import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_action(action, action_space):\n",
    "    # Given a scalar action, return a one-hot encoded action\n",
    "    \n",
    "    return [0 for _ in range(action)] + [1] + [0 for _ in range(action + 1, action_space)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, actions=SIMPLE_MOVEMENT):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(agent, total_rewards, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "    torch.save(agent.STATE_MEM,  f\"STATE_MEM-{run_id}.pt\")\n",
    "    torch.save(agent.ACTION_MEM, f\"ACTION_MEM-{run_id}.pt\")\n",
    "    torch.save(agent.REWARD_MEM, f\"REWARD_MEM-{run_id}.pt\")\n",
    "    torch.save(agent.STATE2_MEM, f\"STATE2_MEM-{run_id}.pt\")\n",
    "    torch.save(agent.DONE_MEM,   f\"DONE_MEM-{run_id}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(training_mode=True, pretrained=False, lr=0.00025, gamma=0.90, exploration_decay=0.99, exploration_min=0.02,\n",
    "        mario_env='SuperMarioBros-1-1-v0', actions=SIMPLE_MOVEMENT, num_episodes=1000, run_id=None):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    #env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    \n",
    "    #env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    #env = SuperMarioBrosEnv()\n",
    "    env = make_env(env, actions)\n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if training_mode and (ep_num % 300) == 0:\n",
    "            save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > 500:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(500)] + \n",
    "                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/marlios-env/lib/python3.10/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/Caskroom/miniconda/base/envs/marlios-env/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/Caskroom/miniconda/base/envs/marlios-env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/Caskroom/miniconda/base/envs/marlios-env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      " 15%|█▍        | 147/1000 [11:11:06<84:34:05, 356.91s/it] "
     ]
    }
   ],
   "source": [
    "run(training_mode=True, pretrained=False, exploration_decay=0.9995, exploration_min=0.05, num_episodes=1000, run_id='long test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'STATE_MEM-default_simple_movement_10000ep_x.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m run_id \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdefault_simple_movement_10000ep_x\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m#run_id='1681869009'\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m run(training_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, run_id\u001b[39m=\u001b[39;49mrun_id)\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, mario_env, actions, num_episodes, run_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m action_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m     15\u001b[0m \u001b[39m#todo: add agent params as a setting/create different agents in diff functions to run \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(state_space\u001b[39m=\u001b[39;49mobservation_space,\n\u001b[1;32m     18\u001b[0m                  action_space\u001b[39m=\u001b[39;49maction_space,\n\u001b[1;32m     19\u001b[0m                  max_memory_size\u001b[39m=\u001b[39;49m\u001b[39m30000\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m                  batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m                  gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m     22\u001b[0m                  lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m     23\u001b[0m                  dropout\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m                  exploration_max\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m                  exploration_min\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m                  exploration_decay\u001b[39m=\u001b[39;49mexploration_decay,\n\u001b[1;32m     27\u001b[0m                  double_dq\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     28\u001b[0m                  pretrained\u001b[39m=\u001b[39;49mpretrained,\n\u001b[1;32m     29\u001b[0m                  run_id\u001b[39m=\u001b[39;49mrun_id)\n\u001b[1;32m     32\u001b[0m \u001b[39m# num_episodes = 10\u001b[39;00m\n\u001b[1;32m     33\u001b[0m env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Documents/CSCI566/marlios/maRLios/toolkit/baseline_model.py:80\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, state_space, action_space, max_memory_size, batch_size, gamma, lr, dropout, exploration_max, exploration_min, exploration_decay, double_dq, pretrained, run_id)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_memory_size \u001b[39m=\u001b[39m max_memory_size\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrained:\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTATE_MEM \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSTATE_MEM-\u001b[39;49m\u001b[39m{\u001b[39;49;00mrun_id\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mACTION_MEM \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mACTION_MEM-\u001b[39m\u001b[39m{\u001b[39;00mrun_id\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mREWARD_MEM \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mREWARD_MEM-\u001b[39m\u001b[39m{\u001b[39;00mrun_id\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages/torch/serialization.py:793\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    791\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 793\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    794\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    795\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    797\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages/torch/serialization.py:273\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 273\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    274\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/marlios-env/lib/python3.10/site-packages/torch/serialization.py:254\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 254\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'STATE_MEM-default_simple_movement_10000ep_x.pt'"
     ]
    }
   ],
   "source": [
    "run_id = 'default_simple_movement_10000ep_x'\n",
    "\n",
    "#run_id='1681869009'\n",
    "\n",
    "run(training_mode=False, pretrained=True, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlios",
   "language": "python",
   "name": "marlios"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
