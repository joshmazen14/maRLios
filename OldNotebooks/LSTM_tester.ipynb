{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.train_marlios import *\n",
    "from toolkit.constants import *\n",
    "from toolkit.train_test_samples import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class DQNSolver(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_actions = 64):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=8, stride=4),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=6, stride=4),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.Conv2d(64, 32, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # Xavier initialization for the convolution layer weights\n",
    "        for layer in self.conv:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "\n",
    "        self.lstm = nn.LSTM(conv_out_size, 64, batch_first=True)\n",
    "        \n",
    "        # takes the output of the convolutions and gets vector to size 32\n",
    "        self.lstm_to_32 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        for layer in self.lstm_to_32:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "       \n",
    "        action_size = 10\n",
    "        self.action_fc = nn.Sequential(\n",
    "            nn.Linear(action_size, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "         # Xavier initialization for the fully connected layer weights\n",
    "        for layer in self.action_fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "        # We take a vector of 5 being the initial action, and 5 being the second action for action size of 10\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(n_actions), # using batch size of 64, for now hard coded\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10), # added a new layer can play with the parameters\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "        for layer in self.fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x, sampled_actions, prev_hidden_state = None):\n",
    "        '''\n",
    "        x - image being passed in as the state\n",
    "        sampled_actions - np.array with n x 8 \n",
    "        prev_hidden_state - tuple of format(hidden, cell) for the hidden states\n",
    "        '''\n",
    "        if prev_hidden_state is None:\n",
    "            # initialize empty hidden state of 0's\n",
    "            h_0 = torch.zeros(1, 1, 64).to(x.device)\n",
    "            c_0 = torch.zeros(1, 1, 64).to(x.device)\n",
    "        else:\n",
    "            h_0, c_0 = prev_hidden_state \n",
    "\n",
    "        big_conv_out = self.conv(x).view(x.size()[0], -1) # has shape of (1, 1024) => (batch, output size)\n",
    "        \n",
    "        # pass to lstm layer, and store output and hidden state\n",
    "        lstm_out, (h_n, c_n) = self.lstm(big_conv_out.unsqueeze(1), (h_0, c_0))\n",
    "        lstm_out = lstm_out.squeeze(1) # remove the sequence length dimension\n",
    "        lstm_out = self.lstm_to_32(lstm_out)\n",
    "\n",
    "        batched_lstm_out = lstm_out.reshape(lstm_out.shape[0], 1, lstm_out.shape[-1]).repeat(1, sampled_actions.shape[-2], 1)\n",
    "\n",
    "        latent_actions = self.action_fc(sampled_actions)\n",
    "\n",
    "        batched_actions = torch.cat((batched_lstm_out, latent_actions), dim=2)\n",
    "\n",
    "        out =  torch.flatten(self.fc(batched_actions), start_dim=1)\n",
    "\n",
    "        return out, (h_n, c_n)\n",
    "\n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, action_space, max_memory_size, batch_size, gamma, lr, state_space,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, double_dq, pretrained, run_id='', n_actions = 64, device=None, init_max_time=500):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "\n",
    "        self.action_space = action_space # this will be a set of actions ie: a subset of TWO_ACTIONS in constants.py\n",
    "        self.n_actions = n_actions # initial number of actions to sample\n",
    "        if device == None:\n",
    "            self.device ='cpu'\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "            elif torch.backends.mps.is_available():\n",
    "                self.device = 'mps'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "\n",
    "        self.subsample_actions()\n",
    "\n",
    "        self.double_dq = double_dq\n",
    "        self.pretrained = pretrained\n",
    "        \n",
    "\n",
    "        # this has been altered as we no longer need to pass the number of actions\n",
    "        self.local_net = DQNSolver(self.state_space, n_actions=n_actions).to(self.device)\n",
    "        self.target_net = DQNSolver(self.state_space, n_actions=n_actions).to(self.device)\n",
    "        \n",
    "        if self.pretrained:\n",
    "            self.local_net.load_state_dict(torch.load(f\"dq1-{run_id}.pt\", map_location=torch.device(self.device)))\n",
    "            self.target_net.load_state_dict(torch.load(f\"dq2-{run_id}.pt\", map_location=torch.device(self.device)))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "        self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "        self.step = 0\n",
    "        self.max_time_per_ep = init_max_time\n",
    "    \n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if self.pretrained:\n",
    "            with open(f\"ending_position-{run_id}.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(f\"num_in_queue-{run_id}.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "\n",
    "        self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "        self.ACTION_MEM = torch.zeros(max_memory_size, 1) # this needs to be a matrix of the actual action taken\n",
    "        self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "        self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "        self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "        self.SPACE_MEM = torch.zeros(max_memory_size, self.n_actions, 10)\n",
    "\n",
    "        # for the lstm layers, i think these need to be on the same device\n",
    "        self.HIDDEN_MEM = torch.zeros(max_memory_size, 1, 1, 64)\n",
    "        self.CELL_MEM = torch.zeros(max_memory_size, 1, 1, 64)\n",
    "        \n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.l2 = nn.MSELoss().to(self.device)\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "        \n",
    "\n",
    "    def subsample_actions(self):\n",
    "        '''\n",
    "        Changes curaction space to be a random sample of what it was\n",
    "        '''\n",
    "\n",
    "        self.cur_action_space = torch.from_numpy(toolkit.action_utils.sample_actions(self.action_space, self.n_actions)).to(torch.float32).to(self.device).unsqueeze(0)\n",
    "    \n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done, hidden_state):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.SPACE_MEM[self.ending_position] = self.cur_action_space\n",
    "        self.HIDDEN_MEM[self.ending_position] = hidden_state[0].float()\n",
    "        self.CELL_MEM[self.ending_position] = hidden_state[1].float()\n",
    "\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Randomly sample 'batch size' experiences\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        \n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]\n",
    "        SPACE = self.SPACE_MEM[idx]\n",
    "\n",
    "        HIDDEN = self.HIDDEN_MEM[idx]\n",
    "        CELL = self.CELL_MEM[idx]\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE, SPACE, HIDDEN, CELL\n",
    "\n",
    "    def act(self, state, prev_hidden_state):\n",
    "        '''\n",
    "        Returns the action index and hidden state\n",
    "        '''\n",
    "        # Epsilon-greedy action\n",
    "        \n",
    "        # increment step\n",
    "        self.step += 1\n",
    "        results, hidden = self.local_net(state.to(self.device), self.cur_action_space, prev_hidden_state)\n",
    "        ind = torch.argmax(results, dim=1) # index of the 'best' action\n",
    "\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            rand_ind = random.randrange(0, self.cur_action_space.shape[1])\n",
    "            ind = torch.tensor(rand_ind).unsqueeze(0) # wiht some probability, choose a random index\n",
    "           \n",
    "        return ind.cpu(), hidden\n",
    "\n",
    "    def copy_model(self):\n",
    "        # Copy local net weights into target net\n",
    "        \n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "\n",
    "    def decay_exploration(self):\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
    "\n",
    "    def decay_lr(self, lr_decay):\n",
    "        self.lr *= lr_decay\n",
    "        self.lr = max(self.lr, 0.000000001)\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = self.lr\n",
    "\n",
    "    def experience_replay(self, debug=False):\n",
    "        \n",
    "        if self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return None\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE, SPACE, HIDDEN, CELL = self.recall()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        SPACE = SPACE.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        HIDDEN = HIDDEN.to(self.device)\n",
    "        CELL = CELL.to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # Double Q-Learning target is Q*(S, A) <- r + Î³ max_a Q_target(S', a)\n",
    "\n",
    "        current, (HIDDEN2, CELL2) = self.local_net(STATE, SPACE, (HIDDEN, CELL))\n",
    "        current = current.gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "    \n",
    "        target, _ = self.target_net(STATE2, SPACE, (HIDDEN2, CELL2))\n",
    "        target = REWARD + torch.mul((self.gamma * target.max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "\n",
    "        loss = self.l1(current, target) # maybe we can play with some L2 loss \n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "        if debug:\n",
    "            return loss.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "env = gym.make(mario_env)\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "state = env.reset()\n",
    "state = torch.Tensor([state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "                state_space=env.observation_space.shape,\n",
    "                action_space=TEST_SET,\n",
    "                max_memory_size=30000,\n",
    "                batch_size=64,\n",
    "                gamma=0.9,\n",
    "                lr=0.000005,\n",
    "                dropout=None,\n",
    "                exploration_max=1,\n",
    "                exploration_min=0.05,\n",
    "                exploration_decay=0.99,\n",
    "                double_dq=True,\n",
    "                pretrained=False,\n",
    "                run_id=None,\n",
    "                n_actions=32,\n",
    "                device='cpu',\n",
    "                init_max_time=100\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([31]),\n",
       " (tensor([[[-0.0070, -0.0175, -0.0658, -0.0457,  0.0699,  0.0139,  0.0391,\n",
       "            -0.0434,  0.0403, -0.0055,  0.0033, -0.0215,  0.0479, -0.0074,\n",
       "             0.0024, -0.0067, -0.0178,  0.0278, -0.0341,  0.0561, -0.0093,\n",
       "            -0.0342, -0.0376, -0.0027, -0.0055, -0.0063, -0.0065,  0.0051,\n",
       "            -0.0128,  0.0467, -0.0388,  0.0133,  0.0023, -0.0299,  0.0153,\n",
       "             0.0465,  0.0173,  0.0216, -0.0231,  0.0085, -0.0512,  0.0714,\n",
       "             0.0460, -0.0215,  0.0103,  0.0088, -0.0157, -0.0393, -0.0261,\n",
       "            -0.0170, -0.0509, -0.0246, -0.0049, -0.0674, -0.0291,  0.0481,\n",
       "             0.0005,  0.0034,  0.0252, -0.0083,  0.0074,  0.0478,  0.0397,\n",
       "            -0.0440]]], grad_fn=<StackBackward0>),\n",
       "  tensor([[[-0.0154, -0.0333, -0.1326, -0.0859,  0.1365,  0.0250,  0.0737,\n",
       "            -0.0884,  0.0777, -0.0120,  0.0071, -0.0422,  0.0969, -0.0153,\n",
       "             0.0045, -0.0132, -0.0367,  0.0514, -0.0694,  0.1054, -0.0181,\n",
       "            -0.0671, -0.0772, -0.0058, -0.0101, -0.0114, -0.0125,  0.0098,\n",
       "            -0.0267,  0.0933, -0.0787,  0.0251,  0.0048, -0.0602,  0.0319,\n",
       "             0.0961,  0.0339,  0.0454, -0.0466,  0.0168, -0.0999,  0.1344,\n",
       "             0.0911, -0.0432,  0.0193,  0.0186, -0.0341, -0.0731, -0.0496,\n",
       "            -0.0348, -0.0984, -0.0465, -0.0095, -0.1284, -0.0654,  0.0968,\n",
       "             0.0010,  0.0075,  0.0448, -0.0174,  0.0136,  0.0997,  0.0819,\n",
       "            -0.0893]]], grad_fn=<StackBackward0>)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(state=state, prev_hidden_state=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
