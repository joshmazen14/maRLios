{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only run once\n",
    "# #!pip install nes-py==0.2.6\n",
    "# !brew update\n",
    "# !brew install ffmpeg\n",
    "# !brew install libsm\n",
    "# !brew install libxext\n",
    "# !brew install mesa\n",
    "# !pip install opencv-python\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from toolkit.gym_env import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.constants import *\n",
    "\n",
    "CONSECUTIVE_ACTIONS = 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, actions=ACTION_SPACE):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(agent, total_rewards, terminal_info, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    with open(f\"terminal_info-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(terminal_info, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "    # torch.save(agent.STATE_MEM,  f\"STATE_MEM-{run_id}.pt\")\n",
    "    # torch.save(agent.ACTION_MEM, f\"ACTION_MEM-{run_id}.pt\")\n",
    "    # torch.save(agent.REWARD_MEM, f\"REWARD_MEM-{run_id}.pt\")\n",
    "    # torch.save(agent.STATE2_MEM, f\"STATE2_MEM-{run_id}.pt\")\n",
    "    # torch.save(agent.DONE_MEM,   f\"DONE_MEM-{run_id}.pt\")\n",
    "    # torch.save(agent.SPACE_MEM,   f\"SPACE_MEM-{run_id}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the act\n",
    "env = gym.make('SuperMarioBros-1-1-v0')\n",
    "env = make_env(env, ACTION_SPACE) # we always need to declare this with ACTION_SPACE\n",
    "agent = DQNAgent(state_space=env.observation_space.shape,\n",
    "                     action_space=TWO_ACTIONS_SET,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1.0 ,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=.995,\n",
    "                     double_dq=True,\n",
    "                     pretrained=False,\n",
    "                     run_id=None,\n",
    "                     n_actions=12)\n",
    "\n",
    "state = env.reset()\n",
    "state = torch.Tensor([state])\n",
    "two_actions_index = agent.act(state)\n",
    "two_actions_vector = agent.cur_action_space[two_actions_index]\n",
    "two_actions = toolkit.action_utils.vec_to_action(two_actions_vector.cpu()) # tuple of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 'down', 'left') 14\n"
     ]
    }
   ],
   "source": [
    "print(two_actions[0], ACTION_TO_INDEX[two_actions[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 185/1000 [40:38<2:59:03, 13.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 110\u001b[0m\n\u001b[1;32m    105\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m    109\u001b[0m \u001b[39m# test it here\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m run(action_space\u001b[39m=\u001b[39;49mSUFFICIENT_ACTIONS, n_actions\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, mario_env, action_space, num_episodes, run_id, n_actions, consecutiveActions)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[1;32m     73\u001b[0m     agent\u001b[39m.\u001b[39mremember(state, two_actions_index, reward, state_next, terminal)\n\u001b[0;32m---> 74\u001b[0m     agent\u001b[39m.\u001b[39;49mexperience_replay()\n\u001b[1;32m     76\u001b[0m state \u001b[39m=\u001b[39m state_next\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m terminal:\n",
      "File \u001b[0;32m~/Programming/marlios/toolkit/marlios_model.py:220\u001b[0m, in \u001b[0;36mDQNAgent.experience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m# Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m target \u001b[39m=\u001b[39m REWARD \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmul((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m \n\u001b[1;32m    217\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_net(STATE2, SPACE)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)), \n\u001b[1;32m    218\u001b[0m                             \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m DONE)\n\u001b[0;32m--> 220\u001b[0m current \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_net(STATE, SPACE)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, ACTION\u001b[39m.\u001b[39mlong()) \u001b[39m# Local net approximation of Q-value\u001b[39;00m\n\u001b[1;32m    223\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1(current, target) \u001b[39m# maybe we can play with some L2 loss \u001b[39;00m\n\u001b[1;32m    224\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m# Compute gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/marlios-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Programming/marlios/toolkit/marlios_model.py:61\u001b[0m, in \u001b[0;36mDQNSolver.forward\u001b[0;34m(self, x, sampled_actions)\u001b[0m\n\u001b[1;32m     58\u001b[0m     sampled_actions \u001b[39m=\u001b[39m sampled_actions\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m batched_actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((batched_conv_out, sampled_actions), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m out \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(batched_actions), start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/marlios-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/marlios-env/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/marlios-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/marlios-env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run(training_mode=True, pretrained=False, lr=0.0001, gamma=0.90, exploration_decay=0.995, exploration_min=0.02,\n",
    "        mario_env='SuperMarioBros-1-1-v0', action_space=TWO_ACTIONS_SET, num_episodes=1000, run_id=None, n_actions=5, consecutiveActions = 2):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    #env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    \n",
    "    #env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    #env = SuperMarioBrosEnv()\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "    observation_space = env.observation_space.shape\n",
    "    \n",
    "    # Change this to be either the training / validation / test set\n",
    "    action_space = TWO_ACTIONS_SET\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[two_actions_index]\n",
    "\n",
    "            two_actions = toolkit.action_utils.vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    state_next = torch.Tensor([state_next])\n",
    "\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if training_mode and (ep_num % 300) == 0:\n",
    "            save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > 500:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(500)] + \n",
    "                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# test it here\n",
    "run(action_space=SUFFICIENT_ACTIONS, n_actions=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
